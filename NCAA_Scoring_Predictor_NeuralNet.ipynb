{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-dense-18-nodes-1581552945\n",
      "0-dense-24-nodes-1581552945\n",
      "0-dense-36-nodes-1581552945\n",
      "1-dense-18-nodes-1581552945\n",
      "1-dense-24-nodes-1581552945\n",
      "1-dense-36-nodes-1581552945\n",
      "2-dense-18-nodes-1581552945\n",
      "2-dense-24-nodes-1581552945\n",
      "2-dense-36-nodes-1581552945\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "dense_layers = [0,1,2]\n",
    "layer_sizes = [18, 24, 36]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        NAME = '{}-dense-{}-nodes-{}'.format(dense_layer, layer_size, int(time.time()))\n",
    "        print(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_data = pd.read_csv('learning_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NameLower</th>\n",
       "      <th>Rank</th>\n",
       "      <th>WinPercentage</th>\n",
       "      <th>BonusPercentage</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Losses</th>\n",
       "      <th>EloPoints</th>\n",
       "      <th>TotalPoints</th>\n",
       "      <th>Big Ten</th>\n",
       "      <th>Big 12</th>\n",
       "      <th>ACC</th>\n",
       "      <th>EIWA</th>\n",
       "      <th>PAC 12</th>\n",
       "      <th>MAC</th>\n",
       "      <th>EWL</th>\n",
       "      <th>EligibilityYear_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spencerlee</td>\n",
       "      <td>1</td>\n",
       "      <td>88.46</td>\n",
       "      <td>82.61</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1618.78</td>\n",
       "      <td>25.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebastianrivera</td>\n",
       "      <td>2</td>\n",
       "      <td>93.75</td>\n",
       "      <td>63.33</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1585.02</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nickpiccininni</td>\n",
       "      <td>3</td>\n",
       "      <td>94.59</td>\n",
       "      <td>80.00</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1581.12</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jackmueller</td>\n",
       "      <td>4</td>\n",
       "      <td>95.45</td>\n",
       "      <td>80.95</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1558.95</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vitaliarujau</td>\n",
       "      <td>5</td>\n",
       "      <td>88.57</td>\n",
       "      <td>61.29</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>1537.20</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         NameLower  Rank  WinPercentage  BonusPercentage  Wins  Losses  \\\n",
       "0       spencerlee     1          88.46            82.61    23       3   \n",
       "1  sebastianrivera     2          93.75            63.33    30       2   \n",
       "2   nickpiccininni     3          94.59            80.00    35       2   \n",
       "3      jackmueller     4          95.45            80.95    21       1   \n",
       "4     vitaliarujau     5          88.57            61.29    31       4   \n",
       "\n",
       "   EloPoints  TotalPoints  Big Ten  Big 12  ACC  EIWA  PAC 12  MAC  EWL  \\\n",
       "0    1618.78         25.5      1.0     0.0  0.0   0.0     0.0  0.0  0.0   \n",
       "1    1585.02         15.5      1.0     0.0  0.0   0.0     0.0  0.0  0.0   \n",
       "2    1581.12         15.5      0.0     1.0  0.0   0.0     0.0  0.0  0.0   \n",
       "3    1558.95         19.0      0.0     0.0  1.0   0.0     0.0  0.0  0.0   \n",
       "4    1537.20         15.5      0.0     0.0  0.0   1.0     0.0  0.0  0.0   \n",
       "\n",
       "   EligibilityYear_num  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    2  \n",
       "3                    2  \n",
       "4                    0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_data.set_index('NameLower',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>WinPercentage</th>\n",
       "      <th>BonusPercentage</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Losses</th>\n",
       "      <th>EloPoints</th>\n",
       "      <th>TotalPoints</th>\n",
       "      <th>Big Ten</th>\n",
       "      <th>Big 12</th>\n",
       "      <th>ACC</th>\n",
       "      <th>EIWA</th>\n",
       "      <th>PAC 12</th>\n",
       "      <th>MAC</th>\n",
       "      <th>EWL</th>\n",
       "      <th>EligibilityYear_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NameLower</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spencerlee</th>\n",
       "      <td>1</td>\n",
       "      <td>88.46</td>\n",
       "      <td>82.61</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1618.78</td>\n",
       "      <td>25.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sebastianrivera</th>\n",
       "      <td>2</td>\n",
       "      <td>93.75</td>\n",
       "      <td>63.33</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1585.02</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nickpiccininni</th>\n",
       "      <td>3</td>\n",
       "      <td>94.59</td>\n",
       "      <td>80.00</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1581.12</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jackmueller</th>\n",
       "      <td>4</td>\n",
       "      <td>95.45</td>\n",
       "      <td>80.95</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1558.95</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitaliarujau</th>\n",
       "      <td>5</td>\n",
       "      <td>88.57</td>\n",
       "      <td>61.29</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>1537.20</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Rank  WinPercentage  BonusPercentage  Wins  Losses  \\\n",
       "NameLower                                                             \n",
       "spencerlee          1          88.46            82.61    23       3   \n",
       "sebastianrivera     2          93.75            63.33    30       2   \n",
       "nickpiccininni      3          94.59            80.00    35       2   \n",
       "jackmueller         4          95.45            80.95    21       1   \n",
       "vitaliarujau        5          88.57            61.29    31       4   \n",
       "\n",
       "                 EloPoints  TotalPoints  Big Ten  Big 12  ACC  EIWA  PAC 12  \\\n",
       "NameLower                                                                     \n",
       "spencerlee         1618.78         25.5      1.0     0.0  0.0   0.0     0.0   \n",
       "sebastianrivera    1585.02         15.5      1.0     0.0  0.0   0.0     0.0   \n",
       "nickpiccininni     1581.12         15.5      0.0     1.0  0.0   0.0     0.0   \n",
       "jackmueller        1558.95         19.0      0.0     0.0  1.0   0.0     0.0   \n",
       "vitaliarujau       1537.20         15.5      0.0     0.0  0.0   1.0     0.0   \n",
       "\n",
       "                 MAC  EWL  EligibilityYear_num  \n",
       "NameLower                                       \n",
       "spencerlee       0.0  0.0                    1  \n",
       "sebastianrivera  0.0  0.0                    1  \n",
       "nickpiccininni   0.0  0.0                    2  \n",
       "jackmueller      0.0  0.0                    2  \n",
       "vitaliarujau     0.0  0.0                    0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = learning_data.drop(['TotalPoints'],axis=1)\n",
    "y = learning_data['TotalPoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fq1228hj\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "X_mm_scaled = mms.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>0.912149</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.801925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.880199</td>\n",
       "      <td>0.682953</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.692909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.896301</td>\n",
       "      <td>0.881122</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.680315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.912785</td>\n",
       "      <td>0.892416</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.608725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.780909</td>\n",
       "      <td>0.658702</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.538491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.744489</td>\n",
       "      <td>0.661674</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.459926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.726088</td>\n",
       "      <td>0.755587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.420434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.661683</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.408325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.574085</td>\n",
       "      <td>0.524489</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.307188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.689093</td>\n",
       "      <td>0.390276</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.297081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.651524</td>\n",
       "      <td>0.414408</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.237309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.424957</td>\n",
       "      <td>0.213148</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.212574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.178455</td>\n",
       "      <td>0.970281</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.200691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.361127</td>\n",
       "      <td>0.227294</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.196655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.303048</td>\n",
       "      <td>0.043272</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.157711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.410197</td>\n",
       "      <td>0.414408</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.149057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.452367</td>\n",
       "      <td>0.595816</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.140209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.225034</td>\n",
       "      <td>0.354612</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.243435</td>\n",
       "      <td>0.291845</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.106658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.267012</td>\n",
       "      <td>0.552782</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146220</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.033099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.041595</td>\n",
       "      <td>0.301593</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.020376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.121526</td>\n",
       "      <td>0.295887</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.011689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.307840</td>\n",
       "      <td>0.188540</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>0.204470</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.904159</td>\n",
       "      <td>0.680932</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.778610</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.820395</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.729721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.896301</td>\n",
       "      <td>0.541488</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.635398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.603412</td>\n",
       "      <td>0.550285</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.557898</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.655933</td>\n",
       "      <td>0.524489</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.249952</td>\n",
       "      <td>0.184855</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.008105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938087</td>\n",
       "      <td>0.801831</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.619284</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.899176</td>\n",
       "      <td>0.623514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.597714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.887292</td>\n",
       "      <td>0.561698</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.544304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.680468</td>\n",
       "      <td>0.881122</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.527092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.680468</td>\n",
       "      <td>0.453162</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.461379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.689093</td>\n",
       "      <td>0.812054</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.434545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.424957</td>\n",
       "      <td>0.269734</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.389208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.589228</td>\n",
       "      <td>0.650618</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.299761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.254552</td>\n",
       "      <td>0.362340</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.279676</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.669542</td>\n",
       "      <td>0.474917</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.255716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.651524</td>\n",
       "      <td>0.766643</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.254650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.477286</td>\n",
       "      <td>0.276866</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.254521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.281196</td>\n",
       "      <td>0.405611</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.219872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.414223</td>\n",
       "      <td>0.500713</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.199916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.424957</td>\n",
       "      <td>0.383024</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.193264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.336400</td>\n",
       "      <td>0.629339</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.188323</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.616638</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.184319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.471152</td>\n",
       "      <td>0.326320</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.172436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.507188</td>\n",
       "      <td>0.250119</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.170402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.405611</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.159261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.598812</td>\n",
       "      <td>0.349620</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.157711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.309948</td>\n",
       "      <td>0.450190</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.142599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.361127</td>\n",
       "      <td>0.254280</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.128746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.288097</td>\n",
       "      <td>0.632549</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.103332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.477286</td>\n",
       "      <td>0.326320</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.088575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.326433</td>\n",
       "      <td>0.574061</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.079534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.233276</td>\n",
       "      <td>0.326320</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.072946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.414223</td>\n",
       "      <td>0.262958</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.018471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.298639</td>\n",
       "      <td>0.204470</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.017825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5    6    7   \\\n",
       "0    0.000000  0.778800  0.912149  0.566667  0.157895  0.801925  1.0  0.0   \n",
       "1    0.018868  0.880199  0.682953  0.800000  0.105263  0.692909  1.0  0.0   \n",
       "2    0.037736  0.896301  0.881122  0.966667  0.105263  0.680315  0.0  1.0   \n",
       "3    0.056604  0.912785  0.892416  0.500000  0.052632  0.608725  0.0  0.0   \n",
       "4    0.075472  0.780909  0.658702  0.833333  0.210526  0.538491  0.0  0.0   \n",
       "5    0.094340  0.744489  0.661674  0.666667  0.210526  0.459926  0.0  0.0   \n",
       "6    0.113208  0.726088  0.755587  1.000000  0.315789  0.420434  1.0  0.0   \n",
       "7    0.132075  0.661683  0.694365  0.733333  0.315789  0.408325  1.0  0.0   \n",
       "8    0.188679  0.574085  0.524489  0.266667  0.210526  0.307188  0.0  0.0   \n",
       "9    0.207547  0.689093  0.390276  0.833333  0.315789  0.297081  0.0  0.0   \n",
       "10   0.245283  0.651524  0.414408  0.700000  0.315789  0.237309  0.0  1.0   \n",
       "11   0.339623  0.424957  0.213148  0.500000  0.473684  0.212574  1.0  0.0   \n",
       "12   0.358491  0.178455  0.970281  0.333333  0.631579  0.200691  0.0  1.0   \n",
       "13   0.377358  0.361127  0.227294  0.466667  0.526316  0.196655  0.0  0.0   \n",
       "14   0.415094  0.303048  0.043272  0.500000  0.631579  0.157711  1.0  0.0   \n",
       "15   0.433962  0.410197  0.414408  0.700000  0.631579  0.149057  0.0  1.0   \n",
       "16   0.452830  0.452367  0.595816  0.633333  0.526316  0.140209  0.0  0.0   \n",
       "17   0.509434  0.225034  0.354612  0.733333  1.000000  0.116088  1.0  0.0   \n",
       "18   0.547170  0.243435  0.291845  0.566667  0.789474  0.106658  0.0  1.0   \n",
       "19   0.566038  0.267012  0.552782  0.500000  0.684211  0.099296  1.0  0.0   \n",
       "20   0.698113  0.000000  0.146220  0.166667  0.631579  0.033099  0.0  0.0   \n",
       "21   0.830189  0.041595  0.301593  0.333333  0.842105  0.020376  0.0  0.0   \n",
       "22   0.867925  0.121526  0.295887  0.233333  0.578947  0.011689  0.0  1.0   \n",
       "23   0.905660  0.307840  0.188540  0.566667  0.684211  0.002131  0.0  0.0   \n",
       "24   0.924528  0.079931  0.204470  0.233333  0.631579  0.000000  1.0  0.0   \n",
       "25   0.018868  0.904159  0.680932  0.433333  0.052632  0.778610  1.0  0.0   \n",
       "26   0.037736  0.820395  0.750000  0.766667  0.157895  0.729721  1.0  0.0   \n",
       "27   0.056604  0.896301  0.541488  0.966667  0.105263  0.635398  0.0  1.0   \n",
       "28   0.075472  0.603412  0.550285  0.566667  0.315789  0.557898  1.0  0.0   \n",
       "29   0.094340  0.655933  0.524489  0.866667  0.368421  0.546500  1.0  0.0   \n",
       "..        ...       ...       ...       ...       ...       ...  ...  ...   \n",
       "258  0.811321  0.249952  0.184855  0.266667  0.473684  0.008105  0.0  1.0   \n",
       "259  0.000000  0.938087  0.801831  0.800000  0.052632  0.619284  1.0  0.0   \n",
       "260  0.018868  0.899176  0.623514  1.000000  0.105263  0.597714  1.0  0.0   \n",
       "261  0.037736  0.887292  0.561698  0.866667  0.105263  0.544304  0.0  1.0   \n",
       "262  0.056604  0.680468  0.881122  0.300000  0.157895  0.527092  0.0  0.0   \n",
       "263  0.075472  0.680468  0.453162  0.633333  0.263158  0.461379  0.0  0.0   \n",
       "264  0.094340  0.689093  0.812054  0.833333  0.315789  0.434545  0.0  0.0   \n",
       "265  0.132075  0.424957  0.269734  0.500000  0.473684  0.389208  1.0  0.0   \n",
       "266  0.150943  0.589228  0.650618  0.900000  0.473684  0.299761  1.0  0.0   \n",
       "267  0.188679  0.254552  0.362340  0.166667  0.368421  0.279676  1.0  0.0   \n",
       "268  0.226415  0.669542  0.474917  0.600000  0.263158  0.255716  0.0  0.0   \n",
       "269  0.245283  0.651524  0.766643  0.700000  0.315789  0.254650  0.0  0.0   \n",
       "270  0.264151  0.477286  0.276866  0.600000  0.473684  0.254521  1.0  0.0   \n",
       "271  0.283019  0.281196  0.405611  0.466667  0.631579  0.219872  1.0  0.0   \n",
       "272  0.301887  0.414223  0.500713  0.633333  0.578947  0.199916  0.0  0.0   \n",
       "273  0.320755  0.424957  0.383024  0.500000  0.473684  0.193264  1.0  0.0   \n",
       "274  0.339623  0.336400  0.629339  0.366667  0.473684  0.188323  1.0  0.0   \n",
       "275  0.358491  0.616638  0.413100  0.866667  0.421053  0.184319  0.0  0.0   \n",
       "276  0.377358  0.471152  0.326320  0.500000  0.421053  0.172436  0.0  0.0   \n",
       "277  0.396226  0.507188  0.250119  0.666667  0.473684  0.170402  0.0  1.0   \n",
       "278  0.433962  0.510638  0.405611  0.966667  0.631579  0.159261  0.0  1.0   \n",
       "279  0.452830  0.598812  0.349620  0.933333  0.473684  0.157711  0.0  0.0   \n",
       "280  0.471698  0.309948  0.450190  0.333333  0.473684  0.142599  0.0  0.0   \n",
       "281  0.490566  0.361127  0.254280  0.533333  0.578947  0.128746  0.0  0.0   \n",
       "282  0.509434  0.288097  0.632549  0.533333  0.684211  0.103332  0.0  0.0   \n",
       "283  0.547170  0.477286  0.326320  0.600000  0.473684  0.088575  0.0  1.0   \n",
       "284  0.584906  0.326433  0.574061  0.600000  0.684211  0.079534  0.0  0.0   \n",
       "285  0.603774  0.233276  0.326320  0.300000  0.526316  0.072946  0.0  0.0   \n",
       "286  0.698113  0.414223  0.262958  0.633333  0.578947  0.018471  0.0  0.0   \n",
       "287  0.716981  0.298639  0.204470  0.666667  0.789474  0.017825  0.0  0.0   \n",
       "\n",
       "      8    9    10   11   12        13  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "3    1.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "4    0.0  1.0  0.0  0.0  0.0  0.000000  \n",
       "5    0.0  0.0  1.0  0.0  0.0  1.000000  \n",
       "6    0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "7    0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "8    1.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "9    0.0  0.0  0.0  1.0  0.0  0.333333  \n",
       "10   0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "11   0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "12   0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "13   0.0  0.0  0.0  1.0  0.0  0.333333  \n",
       "14   0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "15   0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "16   0.0  1.0  0.0  0.0  0.0  0.000000  \n",
       "17   0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "18   0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "19   0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "20   0.0  0.0  0.0  1.0  0.0  0.000000  \n",
       "21   0.0  0.0  0.0  1.0  0.0  0.000000  \n",
       "22   0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "23   0.0  1.0  0.0  0.0  0.0  0.333333  \n",
       "24   0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "25   0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "26   0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "27   0.0  0.0  0.0  0.0  0.0  0.000000  \n",
       "28   0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "29   0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "..   ...  ...  ...  ...  ...       ...  \n",
       "258  0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "259  0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "260  0.0  0.0  0.0  0.0  0.0  0.000000  \n",
       "261  0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "262  0.0  0.0  1.0  0.0  0.0  1.000000  \n",
       "263  0.0  1.0  0.0  0.0  0.0  0.333333  \n",
       "264  0.0  0.0  0.0  1.0  0.0  0.333333  \n",
       "265  0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "266  0.0  0.0  0.0  0.0  0.0  0.000000  \n",
       "267  0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "268  0.0  0.0  0.0  0.0  1.0  1.000000  \n",
       "269  1.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "270  0.0  0.0  0.0  0.0  0.0  0.000000  \n",
       "271  0.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "272  0.0  0.0  0.0  1.0  0.0  0.000000  \n",
       "273  0.0  0.0  0.0  0.0  0.0  0.000000  \n",
       "274  0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "275  0.0  0.0  0.0  0.0  1.0  1.000000  \n",
       "276  0.0  1.0  0.0  0.0  0.0  1.000000  \n",
       "277  0.0  0.0  0.0  0.0  0.0  0.000000  \n",
       "278  0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "279  0.0  1.0  0.0  0.0  0.0  1.000000  \n",
       "280  1.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "281  1.0  0.0  0.0  0.0  0.0  1.000000  \n",
       "282  0.0  0.0  0.0  1.0  0.0  0.000000  \n",
       "283  0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "284  0.0  0.0  0.0  0.0  0.0  0.666667  \n",
       "285  0.0  1.0  0.0  0.0  0.0  1.000000  \n",
       "286  0.0  1.0  0.0  0.0  0.0  1.000000  \n",
       "287  0.0  0.0  0.0  0.0  0.0  0.333333  \n",
       "\n",
       "[288 rows x 14 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_mm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mm_scaled, y, test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fq1228hj\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 36)                540       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                888       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 1,453\n",
      "Trainable params: 1,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(36, input_dim=14, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fq1228hj\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 41 samples\n",
      "WARNING:tensorflow:From C:\\Users\\fq1228hj\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/400\n",
      "160/160 [==============================] - 1s 4ms/sample - loss: 65.2786 - mean_squared_error: 65.2786 - val_loss: 49.3370 - val_mean_squared_error: 49.3370\n",
      "Epoch 2/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 64.3573 - mean_squared_error: 64.3573 - val_loss: 48.4721 - val_mean_squared_error: 48.4721\n",
      "Epoch 3/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 63.2456 - mean_squared_error: 63.2456 - val_loss: 47.4555 - val_mean_squared_error: 47.4555\n",
      "Epoch 4/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 62.0994 - mean_squared_error: 62.0994 - val_loss: 46.3053 - val_mean_squared_error: 46.3053\n",
      "Epoch 5/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 60.7182 - mean_squared_error: 60.7182 - val_loss: 45.1027 - val_mean_squared_error: 45.1027\n",
      "Epoch 6/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 59.3103 - mean_squared_error: 59.3103 - val_loss: 43.7951 - val_mean_squared_error: 43.7951\n",
      "Epoch 7/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 57.7706 - mean_squared_error: 57.7706 - val_loss: 42.4071 - val_mean_squared_error: 42.4071\n",
      "Epoch 8/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 56.1959 - mean_squared_error: 56.1959 - val_loss: 40.9503 - val_mean_squared_error: 40.9503\n",
      "Epoch 9/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 54.3914 - mean_squared_error: 54.3914 - val_loss: 39.3785 - val_mean_squared_error: 39.3785\n",
      "Epoch 10/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 52.6195 - mean_squared_error: 52.6195 - val_loss: 37.7306 - val_mean_squared_error: 37.7306\n",
      "Epoch 11/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 50.7408 - mean_squared_error: 50.7408 - val_loss: 36.0619 - val_mean_squared_error: 36.0619\n",
      "Epoch 12/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 48.9443 - mean_squared_error: 48.9443 - val_loss: 34.4548 - val_mean_squared_error: 34.4548\n",
      "Epoch 13/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 47.1512 - mean_squared_error: 47.1512 - val_loss: 32.9504 - val_mean_squared_error: 32.9504\n",
      "Epoch 14/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 45.3962 - mean_squared_error: 45.3962 - val_loss: 31.6338 - val_mean_squared_error: 31.6338\n",
      "Epoch 15/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 43.9917 - mean_squared_error: 43.9917 - val_loss: 30.4308 - val_mean_squared_error: 30.4308\n",
      "Epoch 16/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 42.5399 - mean_squared_error: 42.5399 - val_loss: 29.3794 - val_mean_squared_error: 29.3794\n",
      "Epoch 17/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 41.2921 - mean_squared_error: 41.2921 - val_loss: 28.4504 - val_mean_squared_error: 28.4504\n",
      "Epoch 18/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 40.1980 - mean_squared_error: 40.1980 - val_loss: 27.6804 - val_mean_squared_error: 27.6804\n",
      "Epoch 19/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 39.3181 - mean_squared_error: 39.3181 - val_loss: 26.9905 - val_mean_squared_error: 26.9905\n",
      "Epoch 20/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 38.6009 - mean_squared_error: 38.6009 - val_loss: 26.3095 - val_mean_squared_error: 26.3095\n",
      "Epoch 21/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 37.8035 - mean_squared_error: 37.8035 - val_loss: 25.6407 - val_mean_squared_error: 25.6407\n",
      "Epoch 22/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 36.9967 - mean_squared_error: 36.9967 - val_loss: 24.9716 - val_mean_squared_error: 24.9716\n",
      "Epoch 23/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 36.3268 - mean_squared_error: 36.3268 - val_loss: 24.2782 - val_mean_squared_error: 24.2782\n",
      "Epoch 24/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 35.5940 - mean_squared_error: 35.5940 - val_loss: 23.5884 - val_mean_squared_error: 23.5884\n",
      "Epoch 25/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 34.7849 - mean_squared_error: 34.7849 - val_loss: 22.8855 - val_mean_squared_error: 22.8855\n",
      "Epoch 26/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 34.0954 - mean_squared_error: 34.0954 - val_loss: 22.1687 - val_mean_squared_error: 22.1687\n",
      "Epoch 27/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 33.3384 - mean_squared_error: 33.3384 - val_loss: 21.4401 - val_mean_squared_error: 21.4401\n",
      "Epoch 28/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 32.6551 - mean_squared_error: 32.6551 - val_loss: 20.7120 - val_mean_squared_error: 20.7120\n",
      "Epoch 29/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 31.8679 - mean_squared_error: 31.8679 - val_loss: 19.9923 - val_mean_squared_error: 19.9923\n",
      "Epoch 30/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 31.1508 - mean_squared_error: 31.1508 - val_loss: 19.2772 - val_mean_squared_error: 19.2772\n",
      "Epoch 31/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 30.4285 - mean_squared_error: 30.4285 - val_loss: 18.5549 - val_mean_squared_error: 18.5549\n",
      "Epoch 32/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 29.6945 - mean_squared_error: 29.6945 - val_loss: 17.8230 - val_mean_squared_error: 17.8230\n",
      "Epoch 33/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 29.0026 - mean_squared_error: 29.0026 - val_loss: 17.0841 - val_mean_squared_error: 17.0841\n",
      "Epoch 34/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 28.1940 - mean_squared_error: 28.1940 - val_loss: 16.3979 - val_mean_squared_error: 16.3979\n",
      "Epoch 35/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 27.4821 - mean_squared_error: 27.4821 - val_loss: 15.6464 - val_mean_squared_error: 15.6464\n",
      "Epoch 36/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 26.6841 - mean_squared_error: 26.6841 - val_loss: 14.8634 - val_mean_squared_error: 14.8634\n",
      "Epoch 37/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 25.9336 - mean_squared_error: 25.9336 - val_loss: 14.0320 - val_mean_squared_error: 14.0320\n",
      "Epoch 38/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 25.0258 - mean_squared_error: 25.0258 - val_loss: 13.2498 - val_mean_squared_error: 13.2498\n",
      "Epoch 39/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 24.2630 - mean_squared_error: 24.2630 - val_loss: 12.5032 - val_mean_squared_error: 12.5032\n",
      "Epoch 40/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 23.4071 - mean_squared_error: 23.4071 - val_loss: 11.8142 - val_mean_squared_error: 11.8142\n",
      "Epoch 41/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 22.6620 - mean_squared_error: 22.6620 - val_loss: 11.1432 - val_mean_squared_error: 11.1432\n",
      "Epoch 42/400\n",
      "160/160 [==============================] - ETA: 0s - loss: 15.8766 - mean_squared_error: 15.87 - 0s 75us/sample - loss: 21.9245 - mean_squared_error: 21.9245 - val_loss: 10.5074 - val_mean_squared_error: 10.5074\n",
      "Epoch 43/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 21.2698 - mean_squared_error: 21.2698 - val_loss: 9.9067 - val_mean_squared_error: 9.9067\n",
      "Epoch 44/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 20.6045 - mean_squared_error: 20.6045 - val_loss: 9.3398 - val_mean_squared_error: 9.3398\n",
      "Epoch 45/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 19.9136 - mean_squared_error: 19.9136 - val_loss: 8.7872 - val_mean_squared_error: 8.7872\n",
      "Epoch 46/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 125us/sample - loss: 19.3363 - mean_squared_error: 19.3363 - val_loss: 8.2959 - val_mean_squared_error: 8.2959\n",
      "Epoch 47/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 18.7518 - mean_squared_error: 18.7518 - val_loss: 7.9023 - val_mean_squared_error: 7.9023\n",
      "Epoch 48/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 18.2699 - mean_squared_error: 18.2699 - val_loss: 7.4979 - val_mean_squared_error: 7.4979\n",
      "Epoch 49/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 17.7655 - mean_squared_error: 17.7655 - val_loss: 7.1755 - val_mean_squared_error: 7.1755\n",
      "Epoch 50/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 17.2277 - mean_squared_error: 17.2277 - val_loss: 6.7992 - val_mean_squared_error: 6.7992\n",
      "Epoch 51/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 16.7885 - mean_squared_error: 16.7885 - val_loss: 6.4508 - val_mean_squared_error: 6.4508\n",
      "Epoch 52/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 16.3624 - mean_squared_error: 16.3624 - val_loss: 6.1439 - val_mean_squared_error: 6.1439\n",
      "Epoch 53/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 15.9089 - mean_squared_error: 15.9089 - val_loss: 5.8574 - val_mean_squared_error: 5.8574\n",
      "Epoch 54/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 15.5320 - mean_squared_error: 15.5320 - val_loss: 5.6360 - val_mean_squared_error: 5.6360\n",
      "Epoch 55/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 15.1498 - mean_squared_error: 15.1498 - val_loss: 5.4117 - val_mean_squared_error: 5.4117\n",
      "Epoch 56/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 14.7827 - mean_squared_error: 14.7827 - val_loss: 5.2104 - val_mean_squared_error: 5.2104\n",
      "Epoch 57/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 14.4648 - mean_squared_error: 14.4648 - val_loss: 5.0172 - val_mean_squared_error: 5.0172\n",
      "Epoch 58/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 14.1406 - mean_squared_error: 14.1406 - val_loss: 4.8854 - val_mean_squared_error: 4.8854\n",
      "Epoch 59/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 13.8255 - mean_squared_error: 13.8255 - val_loss: 4.7459 - val_mean_squared_error: 4.7459\n",
      "Epoch 60/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 13.5605 - mean_squared_error: 13.5605 - val_loss: 4.6426 - val_mean_squared_error: 4.6426\n",
      "Epoch 61/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 13.2787 - mean_squared_error: 13.2787 - val_loss: 4.5147 - val_mean_squared_error: 4.5147\n",
      "Epoch 62/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 12.9789 - mean_squared_error: 12.9789 - val_loss: 4.3939 - val_mean_squared_error: 4.3939\n",
      "Epoch 63/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 12.7266 - mean_squared_error: 12.7266 - val_loss: 4.2918 - val_mean_squared_error: 4.2918\n",
      "Epoch 64/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 12.4889 - mean_squared_error: 12.4889 - val_loss: 4.2373 - val_mean_squared_error: 4.2373\n",
      "Epoch 65/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 12.2263 - mean_squared_error: 12.2263 - val_loss: 4.1731 - val_mean_squared_error: 4.1731\n",
      "Epoch 66/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 11.9914 - mean_squared_error: 11.9914 - val_loss: 4.1331 - val_mean_squared_error: 4.1331\n",
      "Epoch 67/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 11.7694 - mean_squared_error: 11.7694 - val_loss: 4.0592 - val_mean_squared_error: 4.0592\n",
      "Epoch 68/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 11.5756 - mean_squared_error: 11.5756 - val_loss: 3.9882 - val_mean_squared_error: 3.9882\n",
      "Epoch 69/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 11.3868 - mean_squared_error: 11.3868 - val_loss: 3.9358 - val_mean_squared_error: 3.9358\n",
      "Epoch 70/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 11.2262 - mean_squared_error: 11.2262 - val_loss: 3.8857 - val_mean_squared_error: 3.8857\n",
      "Epoch 71/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 11.0701 - mean_squared_error: 11.0701 - val_loss: 3.8305 - val_mean_squared_error: 3.8305\n",
      "Epoch 72/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 10.9414 - mean_squared_error: 10.9414 - val_loss: 3.7939 - val_mean_squared_error: 3.7939\n",
      "Epoch 73/400\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 10.7850 - mean_squared_error: 10.7850 - val_loss: 3.8161 - val_mean_squared_error: 3.8161\n",
      "Epoch 74/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 10.6971 - mean_squared_error: 10.6971 - val_loss: 3.8395 - val_mean_squared_error: 3.8395\n",
      "Epoch 75/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 10.5958 - mean_squared_error: 10.5958 - val_loss: 3.8330 - val_mean_squared_error: 3.8330\n",
      "Epoch 76/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 10.4976 - mean_squared_error: 10.4976 - val_loss: 3.8171 - val_mean_squared_error: 3.8171\n",
      "Epoch 77/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 10.4280 - mean_squared_error: 10.4280 - val_loss: 3.7612 - val_mean_squared_error: 3.7612\n",
      "Epoch 78/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 10.2830 - mean_squared_error: 10.2830 - val_loss: 3.6193 - val_mean_squared_error: 3.6193\n",
      "Epoch 79/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 10.1207 - mean_squared_error: 10.1207 - val_loss: 3.5060 - val_mean_squared_error: 3.5060\n",
      "Epoch 80/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 9.9715 - mean_squared_error: 9.9715 - val_loss: 3.4610 - val_mean_squared_error: 3.4610\n",
      "Epoch 81/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 9.8784 - mean_squared_error: 9.8784 - val_loss: 3.4387 - val_mean_squared_error: 3.4387\n",
      "Epoch 82/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 9.7977 - mean_squared_error: 9.7977 - val_loss: 3.4274 - val_mean_squared_error: 3.4274\n",
      "Epoch 83/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 9.7110 - mean_squared_error: 9.7110 - val_loss: 3.4042 - val_mean_squared_error: 3.4042\n",
      "Epoch 84/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 9.6379 - mean_squared_error: 9.6379 - val_loss: 3.3858 - val_mean_squared_error: 3.3858\n",
      "Epoch 85/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 9.5369 - mean_squared_error: 9.5369 - val_loss: 3.3654 - val_mean_squared_error: 3.3654\n",
      "Epoch 86/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 9.4908 - mean_squared_error: 9.4908 - val_loss: 3.3558 - val_mean_squared_error: 3.3558\n",
      "Epoch 87/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 9.4126 - mean_squared_error: 9.4126 - val_loss: 3.3265 - val_mean_squared_error: 3.3265\n",
      "Epoch 88/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 9.3348 - mean_squared_error: 9.3348 - val_loss: 3.3144 - val_mean_squared_error: 3.3144\n",
      "Epoch 89/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 9.2833 - mean_squared_error: 9.2833 - val_loss: 3.3069 - val_mean_squared_error: 3.3069\n",
      "Epoch 90/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 9.2293 - mean_squared_error: 9.2293 - val_loss: 3.2880 - val_mean_squared_error: 3.2880\n",
      "Epoch 91/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 9.1624 - mean_squared_error: 9.1624 - val_loss: 3.2695 - val_mean_squared_error: 3.2695\n",
      "Epoch 92/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 9.0740 - mean_squared_error: 9.0740 - val_loss: 3.2712 - val_mean_squared_error: 3.2712\n",
      "Epoch 93/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 9.0352 - mean_squared_error: 9.0352 - val_loss: 3.2707 - val_mean_squared_error: 3.2707\n",
      "Epoch 94/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 100us/sample - loss: 8.9619 - mean_squared_error: 8.9619 - val_loss: 3.2610 - val_mean_squared_error: 3.2610\n",
      "Epoch 95/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 8.9069 - mean_squared_error: 8.9069 - val_loss: 3.2673 - val_mean_squared_error: 3.2673\n",
      "Epoch 96/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.8683 - mean_squared_error: 8.8683 - val_loss: 3.2831 - val_mean_squared_error: 3.2831\n",
      "Epoch 97/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.8462 - mean_squared_error: 8.8462 - val_loss: 3.3003 - val_mean_squared_error: 3.3003\n",
      "Epoch 98/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.8084 - mean_squared_error: 8.8084 - val_loss: 3.3133 - val_mean_squared_error: 3.3133\n",
      "Epoch 99/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.7725 - mean_squared_error: 8.7725 - val_loss: 3.3245 - val_mean_squared_error: 3.3245\n",
      "Epoch 100/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.7734 - mean_squared_error: 8.7734 - val_loss: 3.3508 - val_mean_squared_error: 3.3508\n",
      "Epoch 101/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.6786 - mean_squared_error: 8.6786 - val_loss: 3.3428 - val_mean_squared_error: 3.3428\n",
      "Epoch 102/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.6996 - mean_squared_error: 8.6996 - val_loss: 3.3366 - val_mean_squared_error: 3.3366\n",
      "Epoch 103/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.6531 - mean_squared_error: 8.6531 - val_loss: 3.3292 - val_mean_squared_error: 3.3292\n",
      "Epoch 104/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.5902 - mean_squared_error: 8.5902 - val_loss: 3.3212 - val_mean_squared_error: 3.3212\n",
      "Epoch 105/400\n",
      "160/160 [==============================] - 0s 50us/sample - loss: 8.5629 - mean_squared_error: 8.5629 - val_loss: 3.3091 - val_mean_squared_error: 3.3091\n",
      "Epoch 106/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.5395 - mean_squared_error: 8.5395 - val_loss: 3.3050 - val_mean_squared_error: 3.3050\n",
      "Epoch 107/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.5041 - mean_squared_error: 8.5041 - val_loss: 3.3145 - val_mean_squared_error: 3.3145\n",
      "Epoch 108/400\n",
      "160/160 [==============================] - 0s 500us/sample - loss: 8.4564 - mean_squared_error: 8.4564 - val_loss: 3.3397 - val_mean_squared_error: 3.3397\n",
      "Epoch 109/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 8.5221 - mean_squared_error: 8.5221 - val_loss: 3.3626 - val_mean_squared_error: 3.3626\n",
      "Epoch 110/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.4459 - mean_squared_error: 8.4459 - val_loss: 3.2783 - val_mean_squared_error: 3.2783\n",
      "Epoch 111/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 8.3928 - mean_squared_error: 8.3928 - val_loss: 3.2488 - val_mean_squared_error: 3.2488\n",
      "Epoch 112/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.3810 - mean_squared_error: 8.3810 - val_loss: 3.2307 - val_mean_squared_error: 3.2307\n",
      "Epoch 113/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 8.3566 - mean_squared_error: 8.3566 - val_loss: 3.2508 - val_mean_squared_error: 3.2508\n",
      "Epoch 114/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.3500 - mean_squared_error: 8.3500 - val_loss: 3.2664 - val_mean_squared_error: 3.2664\n",
      "Epoch 115/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.3019 - mean_squared_error: 8.3019 - val_loss: 3.3466 - val_mean_squared_error: 3.3466\n",
      "Epoch 116/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.4718 - mean_squared_error: 8.4718 - val_loss: 3.4046 - val_mean_squared_error: 3.4046\n",
      "Epoch 117/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.3946 - mean_squared_error: 8.3946 - val_loss: 3.2473 - val_mean_squared_error: 3.2473\n",
      "Epoch 118/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.2332 - mean_squared_error: 8.2332 - val_loss: 3.2179 - val_mean_squared_error: 3.2179\n",
      "Epoch 119/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.2244 - mean_squared_error: 8.2244 - val_loss: 3.2103 - val_mean_squared_error: 3.2103\n",
      "Epoch 120/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 8.2136 - mean_squared_error: 8.2136 - val_loss: 3.1982 - val_mean_squared_error: 3.1982\n",
      "Epoch 121/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.2025 - mean_squared_error: 8.2025 - val_loss: 3.1709 - val_mean_squared_error: 3.1709\n",
      "Epoch 122/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.2091 - mean_squared_error: 8.2091 - val_loss: 3.1548 - val_mean_squared_error: 3.1548\n",
      "Epoch 123/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.1877 - mean_squared_error: 8.1877 - val_loss: 3.1658 - val_mean_squared_error: 3.1658\n",
      "Epoch 124/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.1438 - mean_squared_error: 8.1438 - val_loss: 3.1606 - val_mean_squared_error: 3.1606\n",
      "Epoch 125/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.0989 - mean_squared_error: 8.0989 - val_loss: 3.1518 - val_mean_squared_error: 3.1518\n",
      "Epoch 126/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.1115 - mean_squared_error: 8.1115 - val_loss: 3.1444 - val_mean_squared_error: 3.1444\n",
      "Epoch 127/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.1048 - mean_squared_error: 8.1048 - val_loss: 3.1553 - val_mean_squared_error: 3.1553\n",
      "Epoch 128/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.1624 - mean_squared_error: 8.1624 - val_loss: 3.1492 - val_mean_squared_error: 3.1492\n",
      "Epoch 129/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.1588 - mean_squared_error: 8.1588 - val_loss: 3.1198 - val_mean_squared_error: 3.1198\n",
      "Epoch 130/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.0988 - mean_squared_error: 8.0988 - val_loss: 3.0999 - val_mean_squared_error: 3.0999\n",
      "Epoch 131/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.0421 - mean_squared_error: 8.0421 - val_loss: 3.1010 - val_mean_squared_error: 3.1010\n",
      "Epoch 132/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 8.0840 - mean_squared_error: 8.0840 - val_loss: 3.1089 - val_mean_squared_error: 3.1089\n",
      "Epoch 133/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 8.0075 - mean_squared_error: 8.0075 - val_loss: 3.1057 - val_mean_squared_error: 3.1057\n",
      "Epoch 134/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 8.0292 - mean_squared_error: 8.0292 - val_loss: 3.1159 - val_mean_squared_error: 3.1159\n",
      "Epoch 135/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.9788 - mean_squared_error: 7.9788 - val_loss: 3.1740 - val_mean_squared_error: 3.1740\n",
      "Epoch 136/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 8.0272 - mean_squared_error: 8.0272 - val_loss: 3.2483 - val_mean_squared_error: 3.2483\n",
      "Epoch 137/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.9854 - mean_squared_error: 7.9854 - val_loss: 3.2138 - val_mean_squared_error: 3.2138\n",
      "Epoch 138/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.9545 - mean_squared_error: 7.9545 - val_loss: 3.1515 - val_mean_squared_error: 3.1515\n",
      "Epoch 139/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.9363 - mean_squared_error: 7.9363 - val_loss: 3.1220 - val_mean_squared_error: 3.1220\n",
      "Epoch 140/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.9317 - mean_squared_error: 7.9317 - val_loss: 3.1087 - val_mean_squared_error: 3.1087\n",
      "Epoch 141/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.9297 - mean_squared_error: 7.9297 - val_loss: 3.1100 - val_mean_squared_error: 3.1100\n",
      "Epoch 142/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.9572 - mean_squared_error: 7.9572 - val_loss: 3.1640 - val_mean_squared_error: 3.1640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.8848 - mean_squared_error: 7.8848 - val_loss: 3.2035 - val_mean_squared_error: 3.2035\n",
      "Epoch 144/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.8731 - mean_squared_error: 7.8731 - val_loss: 3.2412 - val_mean_squared_error: 3.2412\n",
      "Epoch 145/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.9202 - mean_squared_error: 7.9202 - val_loss: 3.2456 - val_mean_squared_error: 3.2456\n",
      "Epoch 146/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.9307 - mean_squared_error: 7.9307 - val_loss: 3.2511 - val_mean_squared_error: 3.2511\n",
      "Epoch 147/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.9170 - mean_squared_error: 7.9170 - val_loss: 3.1938 - val_mean_squared_error: 3.1938\n",
      "Epoch 148/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.8789 - mean_squared_error: 7.8789 - val_loss: 3.2377 - val_mean_squared_error: 3.2377\n",
      "Epoch 149/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.8635 - mean_squared_error: 7.8635 - val_loss: 3.1776 - val_mean_squared_error: 3.1776\n",
      "Epoch 150/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.8119 - mean_squared_error: 7.8119 - val_loss: 3.1226 - val_mean_squared_error: 3.1226\n",
      "Epoch 151/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7887 - mean_squared_error: 7.7887 - val_loss: 3.1032 - val_mean_squared_error: 3.1032\n",
      "Epoch 152/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.7529 - mean_squared_error: 7.7529 - val_loss: 3.0935 - val_mean_squared_error: 3.0935\n",
      "Epoch 153/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7286 - mean_squared_error: 7.7286 - val_loss: 3.0721 - val_mean_squared_error: 3.0721\n",
      "Epoch 154/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.7572 - mean_squared_error: 7.7572 - val_loss: 3.0576 - val_mean_squared_error: 3.0576\n",
      "Epoch 155/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.7731 - mean_squared_error: 7.7731 - val_loss: 3.0257 - val_mean_squared_error: 3.0257\n",
      "Epoch 156/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7273 - mean_squared_error: 7.7273 - val_loss: 3.0059 - val_mean_squared_error: 3.0059\n",
      "Epoch 157/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7160 - mean_squared_error: 7.7160 - val_loss: 2.9966 - val_mean_squared_error: 2.9966\n",
      "Epoch 158/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.6825 - mean_squared_error: 7.6825 - val_loss: 3.0155 - val_mean_squared_error: 3.0155\n",
      "Epoch 159/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.6772 - mean_squared_error: 7.6772 - val_loss: 3.0895 - val_mean_squared_error: 3.0895\n",
      "Epoch 160/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7800 - mean_squared_error: 7.7800 - val_loss: 3.1002 - val_mean_squared_error: 3.1002\n",
      "Epoch 161/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.6508 - mean_squared_error: 7.6508 - val_loss: 2.9553 - val_mean_squared_error: 2.9553\n",
      "Epoch 162/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7146 - mean_squared_error: 7.7146 - val_loss: 2.9183 - val_mean_squared_error: 2.9183\n",
      "Epoch 163/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7332 - mean_squared_error: 7.7332 - val_loss: 2.9418 - val_mean_squared_error: 2.9418\n",
      "Epoch 164/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.8328 - mean_squared_error: 7.8328 - val_loss: 2.9746 - val_mean_squared_error: 2.9746\n",
      "Epoch 165/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.8852 - mean_squared_error: 7.8852 - val_loss: 2.9162 - val_mean_squared_error: 2.9162\n",
      "Epoch 166/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.6535 - mean_squared_error: 7.6535 - val_loss: 2.9114 - val_mean_squared_error: 2.9114\n",
      "Epoch 167/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.5713 - mean_squared_error: 7.5713 - val_loss: 3.2076 - val_mean_squared_error: 3.2076\n",
      "Epoch 168/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.8374 - mean_squared_error: 7.8374 - val_loss: 3.4850 - val_mean_squared_error: 3.4850\n",
      "Epoch 169/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 8.0667 - mean_squared_error: 8.0667 - val_loss: 3.4169 - val_mean_squared_error: 3.4169\n",
      "Epoch 170/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.8668 - mean_squared_error: 7.8668 - val_loss: 3.1565 - val_mean_squared_error: 3.1565\n",
      "Epoch 171/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 7.6170 - mean_squared_error: 7.6170 - val_loss: 2.9469 - val_mean_squared_error: 2.9469\n",
      "Epoch 172/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.5854 - mean_squared_error: 7.5854 - val_loss: 2.9728 - val_mean_squared_error: 2.9728\n",
      "Epoch 173/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.6794 - mean_squared_error: 7.6794 - val_loss: 3.0163 - val_mean_squared_error: 3.0163\n",
      "Epoch 174/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7339 - mean_squared_error: 7.7339 - val_loss: 3.0258 - val_mean_squared_error: 3.0258\n",
      "Epoch 175/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7117 - mean_squared_error: 7.7117 - val_loss: 2.9893 - val_mean_squared_error: 2.9893\n",
      "Epoch 176/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.5083 - mean_squared_error: 7.5083 - val_loss: 3.0407 - val_mean_squared_error: 3.0407\n",
      "Epoch 177/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.6295 - mean_squared_error: 7.6295 - val_loss: 3.3157 - val_mean_squared_error: 3.3157\n",
      "Epoch 178/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7572 - mean_squared_error: 7.7572 - val_loss: 3.4247 - val_mean_squared_error: 3.4247\n",
      "Epoch 179/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.7655 - mean_squared_error: 7.7655 - val_loss: 3.2518 - val_mean_squared_error: 3.2518\n",
      "Epoch 180/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.5707 - mean_squared_error: 7.5707 - val_loss: 2.9670 - val_mean_squared_error: 2.9670\n",
      "Epoch 181/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.4757 - mean_squared_error: 7.4757 - val_loss: 2.8729 - val_mean_squared_error: 2.8729\n",
      "Epoch 182/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.5222 - mean_squared_error: 7.5222 - val_loss: 2.8338 - val_mean_squared_error: 2.8338\n",
      "Epoch 183/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.5700 - mean_squared_error: 7.5700 - val_loss: 2.8157 - val_mean_squared_error: 2.8157\n",
      "Epoch 184/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.5857 - mean_squared_error: 7.5857 - val_loss: 2.8031 - val_mean_squared_error: 2.8031\n",
      "Epoch 185/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.5491 - mean_squared_error: 7.5491 - val_loss: 2.7961 - val_mean_squared_error: 2.7961\n",
      "Epoch 186/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.5381 - mean_squared_error: 7.5381 - val_loss: 2.7795 - val_mean_squared_error: 2.7795\n",
      "Epoch 187/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.5523 - mean_squared_error: 7.5523 - val_loss: 2.7779 - val_mean_squared_error: 2.7779\n",
      "Epoch 188/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.4991 - mean_squared_error: 7.4991 - val_loss: 2.7968 - val_mean_squared_error: 2.7968\n",
      "Epoch 189/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.4878 - mean_squared_error: 7.4878 - val_loss: 2.8961 - val_mean_squared_error: 2.8961\n",
      "Epoch 190/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.4671 - mean_squared_error: 7.4671 - val_loss: 2.9889 - val_mean_squared_error: 2.9889\n",
      "Epoch 191/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 100us/sample - loss: 7.5295 - mean_squared_error: 7.5295 - val_loss: 3.0067 - val_mean_squared_error: 3.0067\n",
      "Epoch 192/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.4743 - mean_squared_error: 7.4743 - val_loss: 2.9007 - val_mean_squared_error: 2.9007\n",
      "Epoch 193/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.4104 - mean_squared_error: 7.4104 - val_loss: 2.8317 - val_mean_squared_error: 2.8317\n",
      "Epoch 194/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.4716 - mean_squared_error: 7.4716 - val_loss: 2.8249 - val_mean_squared_error: 2.8249\n",
      "Epoch 195/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.5667 - mean_squared_error: 7.5667 - val_loss: 2.8621 - val_mean_squared_error: 2.8621\n",
      "Epoch 196/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.6644 - mean_squared_error: 7.6644 - val_loss: 2.8617 - val_mean_squared_error: 2.8617\n",
      "Epoch 197/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.6139 - mean_squared_error: 7.6139 - val_loss: 2.8173 - val_mean_squared_error: 2.8173\n",
      "Epoch 198/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.4976 - mean_squared_error: 7.4976 - val_loss: 2.7838 - val_mean_squared_error: 2.7838\n",
      "Epoch 199/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.3948 - mean_squared_error: 7.3948 - val_loss: 2.7860 - val_mean_squared_error: 2.7860\n",
      "Epoch 200/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.3515 - mean_squared_error: 7.3515 - val_loss: 2.8323 - val_mean_squared_error: 2.8323\n",
      "Epoch 201/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.3410 - mean_squared_error: 7.3410 - val_loss: 2.9289 - val_mean_squared_error: 2.9289\n",
      "Epoch 202/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.3727 - mean_squared_error: 7.3727 - val_loss: 2.9659 - val_mean_squared_error: 2.9659\n",
      "Epoch 203/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.3818 - mean_squared_error: 7.3818 - val_loss: 3.0297 - val_mean_squared_error: 3.0297\n",
      "Epoch 204/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.3976 - mean_squared_error: 7.3976 - val_loss: 3.0207 - val_mean_squared_error: 3.0207\n",
      "Epoch 205/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.3324 - mean_squared_error: 7.3324 - val_loss: 2.9403 - val_mean_squared_error: 2.9403\n",
      "Epoch 206/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.3006 - mean_squared_error: 7.3006 - val_loss: 2.9047 - val_mean_squared_error: 2.9047\n",
      "Epoch 207/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.2891 - mean_squared_error: 7.2891 - val_loss: 2.8870 - val_mean_squared_error: 2.8870\n",
      "Epoch 208/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.2925 - mean_squared_error: 7.2925 - val_loss: 2.8597 - val_mean_squared_error: 2.8597\n",
      "Epoch 209/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.3014 - mean_squared_error: 7.3014 - val_loss: 2.8424 - val_mean_squared_error: 2.8424\n",
      "Epoch 210/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.2993 - mean_squared_error: 7.2993 - val_loss: 2.8555 - val_mean_squared_error: 2.8555\n",
      "Epoch 211/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.2859 - mean_squared_error: 7.2859 - val_loss: 2.9113 - val_mean_squared_error: 2.9113\n",
      "Epoch 212/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.2677 - mean_squared_error: 7.2677 - val_loss: 2.9343 - val_mean_squared_error: 2.9343\n",
      "Epoch 213/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.2657 - mean_squared_error: 7.2657 - val_loss: 2.9390 - val_mean_squared_error: 2.9390\n",
      "Epoch 214/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.2567 - mean_squared_error: 7.2567 - val_loss: 2.8995 - val_mean_squared_error: 2.8995\n",
      "Epoch 215/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.2422 - mean_squared_error: 7.2422 - val_loss: 2.9032 - val_mean_squared_error: 2.9032\n",
      "Epoch 216/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.2151 - mean_squared_error: 7.2151 - val_loss: 2.9498 - val_mean_squared_error: 2.9498\n",
      "Epoch 217/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.2773 - mean_squared_error: 7.2773 - val_loss: 2.9398 - val_mean_squared_error: 2.9398\n",
      "Epoch 218/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.2089 - mean_squared_error: 7.2089 - val_loss: 2.8040 - val_mean_squared_error: 2.8040\n",
      "Epoch 219/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 7.2239 - mean_squared_error: 7.2239 - val_loss: 2.7875 - val_mean_squared_error: 2.7875\n",
      "Epoch 220/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.2543 - mean_squared_error: 7.2543 - val_loss: 2.7866 - val_mean_squared_error: 2.7866\n",
      "Epoch 221/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.2523 - mean_squared_error: 7.2523 - val_loss: 2.7922 - val_mean_squared_error: 2.7922\n",
      "Epoch 222/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.2366 - mean_squared_error: 7.2366 - val_loss: 2.7984 - val_mean_squared_error: 2.7984\n",
      "Epoch 223/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.2156 - mean_squared_error: 7.2156 - val_loss: 2.8500 - val_mean_squared_error: 2.8500\n",
      "Epoch 224/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.1646 - mean_squared_error: 7.1646 - val_loss: 2.8280 - val_mean_squared_error: 2.8280\n",
      "Epoch 225/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.1891 - mean_squared_error: 7.1891 - val_loss: 2.7944 - val_mean_squared_error: 2.7944\n",
      "Epoch 226/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 7.1593 - mean_squared_error: 7.1593 - val_loss: 2.7726 - val_mean_squared_error: 2.7726\n",
      "Epoch 227/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.1489 - mean_squared_error: 7.1489 - val_loss: 2.7328 - val_mean_squared_error: 2.7328\n",
      "Epoch 228/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.1672 - mean_squared_error: 7.1672 - val_loss: 2.7078 - val_mean_squared_error: 2.7078\n",
      "Epoch 229/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.1530 - mean_squared_error: 7.1530 - val_loss: 2.6987 - val_mean_squared_error: 2.6987\n",
      "Epoch 230/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.1651 - mean_squared_error: 7.1651 - val_loss: 2.6882 - val_mean_squared_error: 2.6882\n",
      "Epoch 231/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.1696 - mean_squared_error: 7.1696 - val_loss: 2.7034 - val_mean_squared_error: 2.7034\n",
      "Epoch 232/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.1205 - mean_squared_error: 7.1205 - val_loss: 2.7111 - val_mean_squared_error: 2.7111\n",
      "Epoch 233/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.1406 - mean_squared_error: 7.1406 - val_loss: 2.7368 - val_mean_squared_error: 2.7368\n",
      "Epoch 234/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.1206 - mean_squared_error: 7.1206 - val_loss: 2.7423 - val_mean_squared_error: 2.7423\n",
      "Epoch 235/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.1010 - mean_squared_error: 7.1010 - val_loss: 2.7414 - val_mean_squared_error: 2.7414\n",
      "Epoch 236/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.0881 - mean_squared_error: 7.0881 - val_loss: 2.7478 - val_mean_squared_error: 2.7478\n",
      "Epoch 237/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.0851 - mean_squared_error: 7.0851 - val_loss: 2.7615 - val_mean_squared_error: 2.7615\n",
      "Epoch 238/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.0784 - mean_squared_error: 7.0784 - val_loss: 2.7652 - val_mean_squared_error: 2.7652\n",
      "Epoch 239/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 75us/sample - loss: 7.0636 - mean_squared_error: 7.0636 - val_loss: 2.7692 - val_mean_squared_error: 2.7692\n",
      "Epoch 240/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.0586 - mean_squared_error: 7.0586 - val_loss: 2.7735 - val_mean_squared_error: 2.7735\n",
      "Epoch 241/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.0497 - mean_squared_error: 7.0497 - val_loss: 2.7638 - val_mean_squared_error: 2.7638\n",
      "Epoch 242/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.0574 - mean_squared_error: 7.0574 - val_loss: 2.7691 - val_mean_squared_error: 2.7691\n",
      "Epoch 243/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.0553 - mean_squared_error: 7.0553 - val_loss: 2.7420 - val_mean_squared_error: 2.7420\n",
      "Epoch 244/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.0346 - mean_squared_error: 7.0346 - val_loss: 2.7470 - val_mean_squared_error: 2.7470\n",
      "Epoch 245/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.0566 - mean_squared_error: 7.0566 - val_loss: 2.8012 - val_mean_squared_error: 2.8012\n",
      "Epoch 246/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.0725 - mean_squared_error: 7.0725 - val_loss: 2.8379 - val_mean_squared_error: 2.8379\n",
      "Epoch 247/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.1085 - mean_squared_error: 7.1085 - val_loss: 2.8419 - val_mean_squared_error: 2.8419\n",
      "Epoch 248/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.0691 - mean_squared_error: 7.0691 - val_loss: 2.6904 - val_mean_squared_error: 2.6904\n",
      "Epoch 249/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.9562 - mean_squared_error: 6.9562 - val_loss: 2.6331 - val_mean_squared_error: 2.6331\n",
      "Epoch 250/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.0966 - mean_squared_error: 7.0966 - val_loss: 2.6195 - val_mean_squared_error: 2.6195\n",
      "Epoch 251/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.0876 - mean_squared_error: 7.0876 - val_loss: 2.5947 - val_mean_squared_error: 2.5947\n",
      "Epoch 252/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.9735 - mean_squared_error: 6.9735 - val_loss: 2.6243 - val_mean_squared_error: 2.6243\n",
      "Epoch 253/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.9242 - mean_squared_error: 6.9242 - val_loss: 2.7579 - val_mean_squared_error: 2.7579\n",
      "Epoch 254/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.0025 - mean_squared_error: 7.0025 - val_loss: 2.8533 - val_mean_squared_error: 2.8533\n",
      "Epoch 255/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 7.0404 - mean_squared_error: 7.0404 - val_loss: 2.8475 - val_mean_squared_error: 2.8475\n",
      "Epoch 256/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.0140 - mean_squared_error: 7.0140 - val_loss: 2.7910 - val_mean_squared_error: 2.7910\n",
      "Epoch 257/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.9412 - mean_squared_error: 6.9412 - val_loss: 2.6780 - val_mean_squared_error: 2.6780\n",
      "Epoch 258/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.9044 - mean_squared_error: 6.9044 - val_loss: 2.6318 - val_mean_squared_error: 2.6318\n",
      "Epoch 259/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.0041 - mean_squared_error: 7.0041 - val_loss: 2.6525 - val_mean_squared_error: 2.6525\n",
      "Epoch 260/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 7.1453 - mean_squared_error: 7.1453 - val_loss: 2.6432 - val_mean_squared_error: 2.6432\n",
      "Epoch 261/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 7.0431 - mean_squared_error: 7.0431 - val_loss: 2.5730 - val_mean_squared_error: 2.5730\n",
      "Epoch 262/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.9194 - mean_squared_error: 6.9194 - val_loss: 2.5718 - val_mean_squared_error: 2.5718\n",
      "Epoch 263/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.9221 - mean_squared_error: 6.9221 - val_loss: 2.6149 - val_mean_squared_error: 2.6149\n",
      "Epoch 264/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.8852 - mean_squared_error: 6.8852 - val_loss: 2.6429 - val_mean_squared_error: 2.6429\n",
      "Epoch 265/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.9004 - mean_squared_error: 6.9004 - val_loss: 2.6368 - val_mean_squared_error: 2.6368\n",
      "Epoch 266/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8872 - mean_squared_error: 6.8872 - val_loss: 2.6047 - val_mean_squared_error: 2.6047\n",
      "Epoch 267/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.9321 - mean_squared_error: 6.9321 - val_loss: 2.5479 - val_mean_squared_error: 2.5479\n",
      "Epoch 268/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.8618 - mean_squared_error: 6.8618 - val_loss: 2.5468 - val_mean_squared_error: 2.5468\n",
      "Epoch 269/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8729 - mean_squared_error: 6.8729 - val_loss: 2.5240 - val_mean_squared_error: 2.5240\n",
      "Epoch 270/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8632 - mean_squared_error: 6.8632 - val_loss: 2.5364 - val_mean_squared_error: 2.5364\n",
      "Epoch 271/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.8570 - mean_squared_error: 6.8570 - val_loss: 2.5969 - val_mean_squared_error: 2.5969\n",
      "Epoch 272/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.9111 - mean_squared_error: 6.9111 - val_loss: 2.6718 - val_mean_squared_error: 2.6718\n",
      "Epoch 273/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8222 - mean_squared_error: 6.8222 - val_loss: 2.6488 - val_mean_squared_error: 2.6488\n",
      "Epoch 274/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8191 - mean_squared_error: 6.8191 - val_loss: 2.6200 - val_mean_squared_error: 2.6200\n",
      "Epoch 275/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8199 - mean_squared_error: 6.8199 - val_loss: 2.6058 - val_mean_squared_error: 2.6058\n",
      "Epoch 276/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.8245 - mean_squared_error: 6.8245 - val_loss: 2.5975 - val_mean_squared_error: 2.5975\n",
      "Epoch 277/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8012 - mean_squared_error: 6.8012 - val_loss: 2.6424 - val_mean_squared_error: 2.6424\n",
      "Epoch 278/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.7804 - mean_squared_error: 6.7804 - val_loss: 2.7018 - val_mean_squared_error: 2.7018\n",
      "Epoch 279/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8716 - mean_squared_error: 6.8716 - val_loss: 2.8334 - val_mean_squared_error: 2.8334\n",
      "Epoch 280/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.8847 - mean_squared_error: 6.8847 - val_loss: 2.8218 - val_mean_squared_error: 2.8218\n",
      "Epoch 281/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.9209 - mean_squared_error: 6.9209 - val_loss: 2.8329 - val_mean_squared_error: 2.8329\n",
      "Epoch 282/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8697 - mean_squared_error: 6.8697 - val_loss: 2.7089 - val_mean_squared_error: 2.7089\n",
      "Epoch 283/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.7767 - mean_squared_error: 6.7767 - val_loss: 2.5673 - val_mean_squared_error: 2.5673\n",
      "Epoch 284/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.7525 - mean_squared_error: 6.7525 - val_loss: 2.5141 - val_mean_squared_error: 2.5141\n",
      "Epoch 285/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.8491 - mean_squared_error: 6.8491 - val_loss: 2.5051 - val_mean_squared_error: 2.5051\n",
      "Epoch 286/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.8569 - mean_squared_error: 6.8569 - val_loss: 2.4841 - val_mean_squared_error: 2.4841\n",
      "Epoch 287/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 75us/sample - loss: 6.7927 - mean_squared_error: 6.7927 - val_loss: 2.4951 - val_mean_squared_error: 2.4951\n",
      "Epoch 288/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.7348 - mean_squared_error: 6.7348 - val_loss: 2.5281 - val_mean_squared_error: 2.5281\n",
      "Epoch 289/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.7411 - mean_squared_error: 6.7411 - val_loss: 2.5364 - val_mean_squared_error: 2.5364\n",
      "Epoch 290/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.7164 - mean_squared_error: 6.7164 - val_loss: 2.5367 - val_mean_squared_error: 2.5367\n",
      "Epoch 291/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.7167 - mean_squared_error: 6.7167 - val_loss: 2.5247 - val_mean_squared_error: 2.5247\n",
      "Epoch 292/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.7552 - mean_squared_error: 6.7552 - val_loss: 2.5271 - val_mean_squared_error: 2.5271\n",
      "Epoch 293/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.7313 - mean_squared_error: 6.7313 - val_loss: 2.5538 - val_mean_squared_error: 2.5538\n",
      "Epoch 294/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.7202 - mean_squared_error: 6.7202 - val_loss: 2.6005 - val_mean_squared_error: 2.6005\n",
      "Epoch 295/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.7140 - mean_squared_error: 6.7140 - val_loss: 2.7255 - val_mean_squared_error: 2.7255\n",
      "Epoch 296/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.7633 - mean_squared_error: 6.7633 - val_loss: 2.7484 - val_mean_squared_error: 2.7484\n",
      "Epoch 297/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.7909 - mean_squared_error: 6.7909 - val_loss: 2.7108 - val_mean_squared_error: 2.7108\n",
      "Epoch 298/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.7013 - mean_squared_error: 6.7013 - val_loss: 2.5460 - val_mean_squared_error: 2.5460\n",
      "Epoch 299/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.6379 - mean_squared_error: 6.6379 - val_loss: 2.4989 - val_mean_squared_error: 2.4989\n",
      "Epoch 300/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.7938 - mean_squared_error: 6.7938 - val_loss: 2.5516 - val_mean_squared_error: 2.5516\n",
      "Epoch 301/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.8609 - mean_squared_error: 6.8609 - val_loss: 2.5116 - val_mean_squared_error: 2.5116\n",
      "Epoch 302/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.7951 - mean_squared_error: 6.7951 - val_loss: 2.4942 - val_mean_squared_error: 2.4942\n",
      "Epoch 303/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.6267 - mean_squared_error: 6.6267 - val_loss: 2.5593 - val_mean_squared_error: 2.5593\n",
      "Epoch 304/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.6347 - mean_squared_error: 6.6347 - val_loss: 2.6285 - val_mean_squared_error: 2.6285\n",
      "Epoch 305/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.6830 - mean_squared_error: 6.6830 - val_loss: 2.6284 - val_mean_squared_error: 2.6284\n",
      "Epoch 306/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.6997 - mean_squared_error: 6.6997 - val_loss: 2.5695 - val_mean_squared_error: 2.5695\n",
      "Epoch 307/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.6337 - mean_squared_error: 6.6337 - val_loss: 2.4612 - val_mean_squared_error: 2.4612\n",
      "Epoch 308/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.6321 - mean_squared_error: 6.6321 - val_loss: 2.4086 - val_mean_squared_error: 2.4086\n",
      "Epoch 309/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.6030 - mean_squared_error: 6.6030 - val_loss: 2.4391 - val_mean_squared_error: 2.4391\n",
      "Epoch 310/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.6330 - mean_squared_error: 6.6330 - val_loss: 2.6073 - val_mean_squared_error: 2.6073\n",
      "Epoch 311/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.7708 - mean_squared_error: 6.7708 - val_loss: 2.7618 - val_mean_squared_error: 2.7618\n",
      "Epoch 312/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.7922 - mean_squared_error: 6.7922 - val_loss: 2.5988 - val_mean_squared_error: 2.5988\n",
      "Epoch 313/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.6136 - mean_squared_error: 6.6136 - val_loss: 2.3581 - val_mean_squared_error: 2.3581\n",
      "Epoch 314/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.5820 - mean_squared_error: 6.5820 - val_loss: 2.3362 - val_mean_squared_error: 2.3362\n",
      "Epoch 315/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.6574 - mean_squared_error: 6.6574 - val_loss: 2.3550 - val_mean_squared_error: 2.3550\n",
      "Epoch 316/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.6520 - mean_squared_error: 6.6520 - val_loss: 2.3226 - val_mean_squared_error: 2.3226\n",
      "Epoch 317/400\n",
      "160/160 [==============================] - ETA: 0s - loss: 8.9781 - mean_squared_error: 8.97 - 0s 100us/sample - loss: 6.5586 - mean_squared_error: 6.5586 - val_loss: 2.3459 - val_mean_squared_error: 2.3459\n",
      "Epoch 318/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.5181 - mean_squared_error: 6.5181 - val_loss: 2.3959 - val_mean_squared_error: 2.3959\n",
      "Epoch 319/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.5702 - mean_squared_error: 6.5702 - val_loss: 2.4575 - val_mean_squared_error: 2.4575\n",
      "Epoch 320/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.5762 - mean_squared_error: 6.5762 - val_loss: 2.4292 - val_mean_squared_error: 2.4292\n",
      "Epoch 321/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.5472 - mean_squared_error: 6.5472 - val_loss: 2.3488 - val_mean_squared_error: 2.3488\n",
      "Epoch 322/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.5066 - mean_squared_error: 6.5066 - val_loss: 2.3321 - val_mean_squared_error: 2.3321\n",
      "Epoch 323/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.5872 - mean_squared_error: 6.5872 - val_loss: 2.3234 - val_mean_squared_error: 2.3234\n",
      "Epoch 324/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.5475 - mean_squared_error: 6.5475 - val_loss: 2.3135 - val_mean_squared_error: 2.3135\n",
      "Epoch 325/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4865 - mean_squared_error: 6.4865 - val_loss: 2.3599 - val_mean_squared_error: 2.3599\n",
      "Epoch 326/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4988 - mean_squared_error: 6.4988 - val_loss: 2.3820 - val_mean_squared_error: 2.3820\n",
      "Epoch 327/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.5011 - mean_squared_error: 6.5011 - val_loss: 2.3960 - val_mean_squared_error: 2.3960\n",
      "Epoch 328/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4798 - mean_squared_error: 6.4798 - val_loss: 2.3500 - val_mean_squared_error: 2.3500\n",
      "Epoch 329/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4521 - mean_squared_error: 6.4521 - val_loss: 2.3140 - val_mean_squared_error: 2.3140\n",
      "Epoch 330/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4510 - mean_squared_error: 6.4510 - val_loss: 2.3283 - val_mean_squared_error: 2.3283\n",
      "Epoch 331/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4458 - mean_squared_error: 6.4458 - val_loss: 2.3770 - val_mean_squared_error: 2.3770\n",
      "Epoch 332/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4367 - mean_squared_error: 6.4367 - val_loss: 2.4074 - val_mean_squared_error: 2.4074\n",
      "Epoch 333/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.4445 - mean_squared_error: 6.4445 - val_loss: 2.4046 - val_mean_squared_error: 2.4046\n",
      "Epoch 334/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.4242 - mean_squared_error: 6.4242 - val_loss: 2.4122 - val_mean_squared_error: 2.4122\n",
      "Epoch 335/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 125us/sample - loss: 6.4237 - mean_squared_error: 6.4237 - val_loss: 2.4142 - val_mean_squared_error: 2.4142\n",
      "Epoch 336/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4098 - mean_squared_error: 6.4098 - val_loss: 2.3972 - val_mean_squared_error: 2.3972\n",
      "Epoch 337/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.3975 - mean_squared_error: 6.3975 - val_loss: 2.3795 - val_mean_squared_error: 2.3795\n",
      "Epoch 338/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.3867 - mean_squared_error: 6.3867 - val_loss: 2.3575 - val_mean_squared_error: 2.3575\n",
      "Epoch 339/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.3832 - mean_squared_error: 6.3832 - val_loss: 2.3267 - val_mean_squared_error: 2.3267\n",
      "Epoch 340/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.4216 - mean_squared_error: 6.4216 - val_loss: 2.3129 - val_mean_squared_error: 2.3129\n",
      "Epoch 341/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4330 - mean_squared_error: 6.4330 - val_loss: 2.3120 - val_mean_squared_error: 2.3120\n",
      "Epoch 342/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.3895 - mean_squared_error: 6.3895 - val_loss: 2.3412 - val_mean_squared_error: 2.3412\n",
      "Epoch 343/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.3224 - mean_squared_error: 6.3224 - val_loss: 2.4800 - val_mean_squared_error: 2.4800\n",
      "Epoch 344/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4392 - mean_squared_error: 6.4392 - val_loss: 2.6856 - val_mean_squared_error: 2.6856\n",
      "Epoch 345/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.5454 - mean_squared_error: 6.5454 - val_loss: 2.7675 - val_mean_squared_error: 2.7675\n",
      "Epoch 346/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.6354 - mean_squared_error: 6.6354 - val_loss: 2.7012 - val_mean_squared_error: 2.7012\n",
      "Epoch 347/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.5716 - mean_squared_error: 6.5716 - val_loss: 2.4566 - val_mean_squared_error: 2.4566\n",
      "Epoch 348/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.3622 - mean_squared_error: 6.3622 - val_loss: 2.3559 - val_mean_squared_error: 2.3559\n",
      "Epoch 349/400\n",
      "160/160 [==============================] - 0s 175us/sample - loss: 6.3018 - mean_squared_error: 6.3018 - val_loss: 2.3199 - val_mean_squared_error: 2.3199\n",
      "Epoch 350/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.3863 - mean_squared_error: 6.3864 - val_loss: 2.3108 - val_mean_squared_error: 2.3108\n",
      "Epoch 351/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.4295 - mean_squared_error: 6.4295 - val_loss: 2.2892 - val_mean_squared_error: 2.2892\n",
      "Epoch 352/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.3922 - mean_squared_error: 6.3922 - val_loss: 2.2727 - val_mean_squared_error: 2.2727\n",
      "Epoch 353/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.3504 - mean_squared_error: 6.3504 - val_loss: 2.2574 - val_mean_squared_error: 2.2574\n",
      "Epoch 354/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.3261 - mean_squared_error: 6.3261 - val_loss: 2.2435 - val_mean_squared_error: 2.2435\n",
      "Epoch 355/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.3000 - mean_squared_error: 6.3000 - val_loss: 2.2462 - val_mean_squared_error: 2.2462\n",
      "Epoch 356/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2891 - mean_squared_error: 6.2891 - val_loss: 2.2431 - val_mean_squared_error: 2.2431\n",
      "Epoch 357/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2888 - mean_squared_error: 6.2888 - val_loss: 2.2211 - val_mean_squared_error: 2.2211\n",
      "Epoch 358/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2697 - mean_squared_error: 6.2697 - val_loss: 2.2213 - val_mean_squared_error: 2.2213\n",
      "Epoch 359/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2850 - mean_squared_error: 6.2850 - val_loss: 2.2221 - val_mean_squared_error: 2.2221\n",
      "Epoch 360/400\n",
      "160/160 [==============================] - 0s 475us/sample - loss: 6.2816 - mean_squared_error: 6.2816 - val_loss: 2.2272 - val_mean_squared_error: 2.2272\n",
      "Epoch 361/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.2375 - mean_squared_error: 6.2375 - val_loss: 2.2441 - val_mean_squared_error: 2.2441\n",
      "Epoch 362/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2426 - mean_squared_error: 6.2426 - val_loss: 2.2831 - val_mean_squared_error: 2.2831\n",
      "Epoch 363/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2377 - mean_squared_error: 6.2377 - val_loss: 2.3022 - val_mean_squared_error: 2.3022\n",
      "Epoch 364/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2398 - mean_squared_error: 6.2398 - val_loss: 2.3207 - val_mean_squared_error: 2.3207\n",
      "Epoch 365/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.2415 - mean_squared_error: 6.2415 - val_loss: 2.3268 - val_mean_squared_error: 2.3268\n",
      "Epoch 366/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.2304 - mean_squared_error: 6.2304 - val_loss: 2.3185 - val_mean_squared_error: 2.3185\n",
      "Epoch 367/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2425 - mean_squared_error: 6.2425 - val_loss: 2.3050 - val_mean_squared_error: 2.3050\n",
      "Epoch 368/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2147 - mean_squared_error: 6.2147 - val_loss: 2.3224 - val_mean_squared_error: 2.3224\n",
      "Epoch 369/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.2346 - mean_squared_error: 6.2346 - val_loss: 2.3260 - val_mean_squared_error: 2.3260\n",
      "Epoch 370/400\n",
      "160/160 [==============================] - ETA: 0s - loss: 6.3972 - mean_squared_error: 6.39 - 0s 100us/sample - loss: 6.2586 - mean_squared_error: 6.2586 - val_loss: 2.4556 - val_mean_squared_error: 2.4556\n",
      "Epoch 371/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.3243 - mean_squared_error: 6.3243 - val_loss: 2.4652 - val_mean_squared_error: 2.4652\n",
      "Epoch 372/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.2399 - mean_squared_error: 6.2399 - val_loss: 2.2984 - val_mean_squared_error: 2.2984\n",
      "Epoch 373/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.1618 - mean_squared_error: 6.1618 - val_loss: 2.2188 - val_mean_squared_error: 2.2188\n",
      "Epoch 374/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.1697 - mean_squared_error: 6.1697 - val_loss: 2.2170 - val_mean_squared_error: 2.2170\n",
      "Epoch 375/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.2066 - mean_squared_error: 6.2066 - val_loss: 2.2306 - val_mean_squared_error: 2.2306\n",
      "Epoch 376/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.2345 - mean_squared_error: 6.2345 - val_loss: 2.2294 - val_mean_squared_error: 2.2294\n",
      "Epoch 377/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.1956 - mean_squared_error: 6.1956 - val_loss: 2.2453 - val_mean_squared_error: 2.2453\n",
      "Epoch 378/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.1296 - mean_squared_error: 6.1296 - val_loss: 2.3016 - val_mean_squared_error: 2.3016\n",
      "Epoch 379/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.1519 - mean_squared_error: 6.1519 - val_loss: 2.3669 - val_mean_squared_error: 2.3669\n",
      "Epoch 380/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.1573 - mean_squared_error: 6.1573 - val_loss: 2.3697 - val_mean_squared_error: 2.3697\n",
      "Epoch 381/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.1568 - mean_squared_error: 6.1568 - val_loss: 2.3425 - val_mean_squared_error: 2.3425\n",
      "Epoch 382/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.1319 - mean_squared_error: 6.1319 - val_loss: 2.3354 - val_mean_squared_error: 2.3354\n",
      "Epoch 383/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 125us/sample - loss: 6.1292 - mean_squared_error: 6.1292 - val_loss: 2.2940 - val_mean_squared_error: 2.2940\n",
      "Epoch 384/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.1178 - mean_squared_error: 6.1178 - val_loss: 2.2659 - val_mean_squared_error: 2.2659\n",
      "Epoch 385/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.1262 - mean_squared_error: 6.1262 - val_loss: 2.2654 - val_mean_squared_error: 2.2654\n",
      "Epoch 386/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.1000 - mean_squared_error: 6.1000 - val_loss: 2.2153 - val_mean_squared_error: 2.2153\n",
      "Epoch 387/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.1030 - mean_squared_error: 6.1030 - val_loss: 2.1846 - val_mean_squared_error: 2.1846\n",
      "Epoch 388/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.1115 - mean_squared_error: 6.1115 - val_loss: 2.1717 - val_mean_squared_error: 2.1717\n",
      "Epoch 389/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.0912 - mean_squared_error: 6.0912 - val_loss: 2.1866 - val_mean_squared_error: 2.1866\n",
      "Epoch 390/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.1510 - mean_squared_error: 6.1510 - val_loss: 2.1853 - val_mean_squared_error: 2.1853\n",
      "Epoch 391/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.1210 - mean_squared_error: 6.1210 - val_loss: 2.1840 - val_mean_squared_error: 2.1840\n",
      "Epoch 392/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.0582 - mean_squared_error: 6.0582 - val_loss: 2.2088 - val_mean_squared_error: 2.2088\n",
      "Epoch 393/400\n",
      "160/160 [==============================] - 0s 75us/sample - loss: 6.0726 - mean_squared_error: 6.0726 - val_loss: 2.2361 - val_mean_squared_error: 2.2361\n",
      "Epoch 394/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.0686 - mean_squared_error: 6.0686 - val_loss: 2.2036 - val_mean_squared_error: 2.2036\n",
      "Epoch 395/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.0966 - mean_squared_error: 6.0966 - val_loss: 2.1950 - val_mean_squared_error: 2.1950\n",
      "Epoch 396/400\n",
      "160/160 [==============================] - 0s 125us/sample - loss: 6.0422 - mean_squared_error: 6.0422 - val_loss: 2.2040 - val_mean_squared_error: 2.2040\n",
      "Epoch 397/400\n",
      "160/160 [==============================] - 0s 150us/sample - loss: 6.0321 - mean_squared_error: 6.0321 - val_loss: 2.2316 - val_mean_squared_error: 2.2316\n",
      "Epoch 398/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.0268 - mean_squared_error: 6.0268 - val_loss: 2.2739 - val_mean_squared_error: 2.2739\n",
      "Epoch 399/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.0419 - mean_squared_error: 6.0419 - val_loss: 2.2940 - val_mean_squared_error: 2.2940\n",
      "Epoch 400/400\n",
      "160/160 [==============================] - 0s 100us/sample - loss: 6.0384 - mean_squared_error: 6.0384 - val_loss: 2.3255 - val_mean_squared_error: 2.3255\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=400, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(preds)):\n",
    "    if preds[i] < 0:\n",
    "        preds[i] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Predicted'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999675459106043\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(metrics.mean_squared_error(y_test,list(df1['Predicted']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fq1228hj\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(27.125, 0.5, 'Predicted Y')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.jointplot(list(df1['TotalPoints']), list(df1['Predicted']))\n",
    "p.set_axis_labels('x', 'y', fontsize=16)\n",
    "p.ax_joint.set_xlabel('Y Test', fontweight='bold')\n",
    "p.ax_joint.set_ylabel('Predicted Y', fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TotalPoints  Predicted\n",
      "NameLower                                  \n",
      "brycewest                    0.0   0.139601\n",
      "devinskatzka                 8.5   5.746366\n",
      "josephmckenna               19.5  18.774614\n",
      "mitchmoore                   2.5   1.670608\n",
      "coleverner                   0.5   0.562828\n",
      "brycesteiert                 8.5   2.631788\n",
      "sawyerroot                   0.0   0.427669\n",
      "mattkolodzik                10.5  12.214272\n",
      "jasonnolf                   27.0  27.189846\n",
      "tomsleigh                    2.0   1.746610\n",
      "tateorndorff                 2.0   1.074408\n",
      "zachelam                     5.0   0.980475\n",
      "seanfausz                    4.0   3.050013\n",
      "jordanwood                  13.0  10.948646\n",
      "danreed                      0.5   0.277264\n",
      "anthonycefolo                0.0   0.165493\n",
      "carsonbrolsma                0.5   0.735767\n",
      "colstondiblasi               0.0   0.106933\n",
      "benharvey                    2.0   2.064315\n",
      "mylesmartin                 18.0  21.550619\n",
      "chadred                      8.5   2.928141\n",
      "emeryparker                 12.5  11.900086\n",
      "dakotageer                   8.0   6.644528\n",
      "jakejakobsen                 1.5   0.411371\n",
      "antoniopelusi                0.0   0.247267\n",
      "bransonashworth              2.0   5.077056\n",
      "sebastianrivera             15.5  20.237062\n",
      "justinthomas                 2.0   1.163407\n",
      "joeygunther                  1.0   0.795247\n",
      "anthonyartalona              3.0   1.466654\n",
      "samstoll                     2.0   3.105243\n",
      "garyjoint                    0.5   0.244725\n",
      "christianpagdilao            8.0   2.548981\n",
      "dominickdemas               14.5  10.833343\n",
      "brentfleetwood               3.0   2.464314\n",
      "davionjeffries               1.0   2.239680\n",
      "joshfinesilver               0.0   0.569631\n",
      "patrickbrucki               12.5  12.353139\n",
      "carymiller                   0.0   0.456572\n",
      "gordonwolf                   1.5   1.085487\n",
      "matthewfindlay               4.5   3.119766\n",
      "maxlyon                      0.5   1.112781\n",
      "bradyberge                   1.5   2.634242\n",
      "jacobschwarm                 3.0   1.000785\n",
      "nicksuriano                 24.0  19.271429\n",
      "matthewzovistoski            0.0   0.250030\n",
      "seannickell                  1.5   1.659823\n",
      "lukeweiland                  1.0   0.973659\n",
      "sa'derianperry               1.0   1.729242\n",
      "amarveerdhesi               17.5  10.155530\n",
      "justinoliver                 1.5   3.140357\n",
      "joshheil                     3.0   1.374217\n",
      "loganmassa                   5.5   8.899837\n",
      "coltonmckiernan              0.0   0.344876\n",
      "josephgrello                 2.0   1.568720\n",
      "loganparks                   2.0   0.501160\n",
      "joshmcclure                  0.0   0.416687\n",
      "drewhildebrandt              1.0   0.684067\n",
      "bonickal                    28.0  26.860281\n",
      "ryanchristensen              1.0   1.791831\n",
      "garywayne harding            0.0   1.517238\n",
      "te'shancampbell              4.0   2.994870\n",
      "jarrettjacques               0.5   0.757561\n",
      "ericschultz                  1.5   1.617831\n",
      "joshhumphreys                3.0   3.013589\n",
      "haydenhidlay                15.5  16.024681\n",
      "patriciolugo                 5.5   5.039198\n",
      "joshmaruca                   0.5   0.904781\n",
      "isaiahwhite                 10.5   9.258821\n",
      "cashwilcke                   2.5   3.069938\n",
      "tannerskidgel                1.5   0.615804\n",
      "joeygoodhart                 1.0   1.146306\n",
      "ethansmith                   2.0   0.998627\n",
      "mattfinesilver               1.0   2.062739\n",
      "travisstefanik               0.0   0.214894\n",
      "chancemarsteller            16.0  15.873498\n",
      "requirvan der merwe          1.5   1.492561\n",
      "marioguillen                 0.0   0.256112\n",
      "yiannidiakomihalis          21.0  24.296795\n",
      "tatesamuelson                1.5   0.886275\n",
      "kalebyoung                  13.0   9.250815\n",
      "noahstewart                  0.5   0.492012\n",
      "ethanlaird                   1.0   0.453633\n",
      "acheadlee                    2.5   0.862131\n",
      "johnvan brill                4.5   2.977253\n",
      "cortlandtschuyler            0.0   0.222774\n",
      "zackzavatsky                 5.5   7.596226\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalPoints</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NameLower</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brycewest</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devinskatzka</th>\n",
       "      <td>8.5</td>\n",
       "      <td>5.746366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>josephmckenna</th>\n",
       "      <td>19.5</td>\n",
       "      <td>18.774614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitchmoore</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1.670608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coleverner</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.562828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brycesteiert</th>\n",
       "      <td>8.5</td>\n",
       "      <td>2.631788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sawyerroot</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mattkolodzik</th>\n",
       "      <td>10.5</td>\n",
       "      <td>12.214272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jasonnolf</th>\n",
       "      <td>27.0</td>\n",
       "      <td>27.189846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tomsleigh</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.746610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tateorndorff</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.074408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zachelam</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.980475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seanfausz</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.050013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jordanwood</th>\n",
       "      <td>13.0</td>\n",
       "      <td>10.948646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danreed</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.277264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthonycefolo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carsonbrolsma</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.735767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colstondiblasi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benharvey</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.064315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mylesmartin</th>\n",
       "      <td>18.0</td>\n",
       "      <td>21.550619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chadred</th>\n",
       "      <td>8.5</td>\n",
       "      <td>2.928141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emeryparker</th>\n",
       "      <td>12.5</td>\n",
       "      <td>11.900086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dakotageer</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.644528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jakejakobsen</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.411371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antoniopelusi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bransonashworth</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.077056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sebastianrivera</th>\n",
       "      <td>15.5</td>\n",
       "      <td>20.237062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>justinthomas</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.163407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joeygunther</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthonyartalona</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.466654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drewhildebrandt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonickal</th>\n",
       "      <td>28.0</td>\n",
       "      <td>26.860281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ryanchristensen</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.791831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garywayne harding</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.517238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>te'shancampbell</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.994870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jarrettjacques</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.757561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ericschultz</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.617831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joshhumphreys</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.013589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haydenhidlay</th>\n",
       "      <td>15.5</td>\n",
       "      <td>16.024681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patriciolugo</th>\n",
       "      <td>5.5</td>\n",
       "      <td>5.039198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joshmaruca</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.904781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isaiahwhite</th>\n",
       "      <td>10.5</td>\n",
       "      <td>9.258821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cashwilcke</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.069938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tannerskidgel</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.615804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joeygoodhart</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.146306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethansmith</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.998627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mattfinesilver</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.062739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travisstefanik</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chancemarsteller</th>\n",
       "      <td>16.0</td>\n",
       "      <td>15.873498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>requirvan der merwe</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.492561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marioguillen</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yiannidiakomihalis</th>\n",
       "      <td>21.0</td>\n",
       "      <td>24.296795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tatesamuelson</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.886275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kalebyoung</th>\n",
       "      <td>13.0</td>\n",
       "      <td>9.250815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noahstewart</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.492012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethanlaird</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.453633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acheadlee</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.862131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>johnvan brill</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.977253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cortlandtschuyler</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zackzavatsky</th>\n",
       "      <td>5.5</td>\n",
       "      <td>7.596226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TotalPoints  Predicted\n",
       "NameLower                                  \n",
       "brycewest                    0.0   0.139601\n",
       "devinskatzka                 8.5   5.746366\n",
       "josephmckenna               19.5  18.774614\n",
       "mitchmoore                   2.5   1.670608\n",
       "coleverner                   0.5   0.562828\n",
       "brycesteiert                 8.5   2.631788\n",
       "sawyerroot                   0.0   0.427669\n",
       "mattkolodzik                10.5  12.214272\n",
       "jasonnolf                   27.0  27.189846\n",
       "tomsleigh                    2.0   1.746610\n",
       "tateorndorff                 2.0   1.074408\n",
       "zachelam                     5.0   0.980475\n",
       "seanfausz                    4.0   3.050013\n",
       "jordanwood                  13.0  10.948646\n",
       "danreed                      0.5   0.277264\n",
       "anthonycefolo                0.0   0.165493\n",
       "carsonbrolsma                0.5   0.735767\n",
       "colstondiblasi               0.0   0.106933\n",
       "benharvey                    2.0   2.064315\n",
       "mylesmartin                 18.0  21.550619\n",
       "chadred                      8.5   2.928141\n",
       "emeryparker                 12.5  11.900086\n",
       "dakotageer                   8.0   6.644528\n",
       "jakejakobsen                 1.5   0.411371\n",
       "antoniopelusi                0.0   0.247267\n",
       "bransonashworth              2.0   5.077056\n",
       "sebastianrivera             15.5  20.237062\n",
       "justinthomas                 2.0   1.163407\n",
       "joeygunther                  1.0   0.795247\n",
       "anthonyartalona              3.0   1.466654\n",
       "...                          ...        ...\n",
       "drewhildebrandt              1.0   0.684067\n",
       "bonickal                    28.0  26.860281\n",
       "ryanchristensen              1.0   1.791831\n",
       "garywayne harding            0.0   1.517238\n",
       "te'shancampbell              4.0   2.994870\n",
       "jarrettjacques               0.5   0.757561\n",
       "ericschultz                  1.5   1.617831\n",
       "joshhumphreys                3.0   3.013589\n",
       "haydenhidlay                15.5  16.024681\n",
       "patriciolugo                 5.5   5.039198\n",
       "joshmaruca                   0.5   0.904781\n",
       "isaiahwhite                 10.5   9.258821\n",
       "cashwilcke                   2.5   3.069938\n",
       "tannerskidgel                1.5   0.615804\n",
       "joeygoodhart                 1.0   1.146306\n",
       "ethansmith                   2.0   0.998627\n",
       "mattfinesilver               1.0   2.062739\n",
       "travisstefanik               0.0   0.214894\n",
       "chancemarsteller            16.0  15.873498\n",
       "requirvan der merwe          1.5   1.492561\n",
       "marioguillen                 0.0   0.256112\n",
       "yiannidiakomihalis          21.0  24.296795\n",
       "tatesamuelson                1.5   0.886275\n",
       "kalebyoung                  13.0   9.250815\n",
       "noahstewart                  0.5   0.492012\n",
       "ethanlaird                   1.0   0.453633\n",
       "acheadlee                    2.5   0.862131\n",
       "johnvan brill                4.5   2.977253\n",
       "cortlandtschuyler            0.0   0.222774\n",
       "zackzavatsky                 5.5   7.596226\n",
       "\n",
       "[87 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
